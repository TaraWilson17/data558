{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Homework 7 </center>\n",
    "<center> Tara Wilson </center>\n",
    "<center> DATA 558 </center>\n",
    "<center> May 25, 2019 </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from numpy.linalg import inv\n",
    "import scipy.linalg\n",
    "import random\n",
    "import copy \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import LinearSVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Study Lab #7 and do exercises therin**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See seperate submission attachment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this problem you will implement in Python a first version of your own kernerl support vector machine with the smoothed hinge loss.**  \n",
    "\n",
    "The kernel support vector maching with the smoothed hinge loss writes as\n",
    "$min F(\\alpha) := \\frac {1}{n} \\sum_{i=1}^{n} l_{hh}(y_i,(K\\alpha)_i) + \\lambda \\alpha^T K \\alpha$    \n",
    "\n",
    "where $K(\\alpha)_i$ is the *i*th entry in the vector $K\\alpha$,  \n",
    "\n",
    "$\\begin{equation}\n",
    "l_{hh}(y, t) := \\begin{cases}\n",
    "    0 \\text{    if    } yt > 1 + h \\\\  \n",
    "    \\frac{(1+h-yt)^2}{4h} \\text{    if    } |1 - yt| \\leq h \\\\  \n",
    "    1 - yt \\text{    if    } yt < 1 - h  \n",
    "    \\end{cases}\n",
    "\\end{equation}$  \n",
    "\n",
    "and h = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the gradient $\\nabla(\\alpha)$ of F**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve for the gradient of F we will need to calculate the derivative:  \n",
    "$\\nabla F(\\alpha) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\alpha} l_{hh}(y_i, (K\\alpha)_i) + \\frac{\\partial}{\\partial \\alpha} \\lambda \\alpha^T K \\alpha$  \n",
    "\n",
    "I will break this in to two terms:  \n",
    "$g'(\\alpha) = \\frac{\\partial}{\\partial \\alpha} l_{hh}(y_i, (K\\alpha)_i)$  \n",
    "$h'(\\alpha) = \\frac{\\partial}{\\partial \\alpha} \\lambda \\alpha^T K \\alpha$ \n",
    "\n",
    "First, solving for $g'(\\alpha)$, we have several cases:  \n",
    "$\\begin{equation}\n",
    "l_{hh}(y_i, (K\\alpha)_i) := \\begin{cases}\n",
    "    0 \\text{    if    } y_i(K\\alpha)_i > 1 + h \\\\  \n",
    "    \\frac{(1+h-y_i(K\\alpha)_i)^2}{4h} \\text{    if    } |1 - y_i(K\\alpha)_i| \\leq h \\\\  \n",
    "    1 - y_i(K\\alpha)_i \\text{    if    } y_i(K\\alpha)_i < 1 - h  \n",
    "    \\end{cases}\n",
    "\\end{equation}$  \n",
    "\n",
    "We can solve for the derivative of each of these cases.  \n",
    "\n",
    "First, we know that $\\frac{\\partial}{\\partial \\alpha} 0 = 0$  \n",
    "\n",
    "Next, we solve for $\\frac{\\partial}{\\partial \\alpha} \\frac{(1+h-y_i(K\\alpha)_i)^2}{4h}$  \n",
    "We must use the division rule $\\frac {a(x)}{b(x)} = a(x)b'(x) - a'(x)b(x)$:  \n",
    "$\\frac{\\partial}{\\partial \\alpha} \\frac{(1+h-y_i(K\\alpha)_i)^2}{4h} = (1 + h - y_i(K\\alpha)_i)^2 * 0 - \\frac{1}{4h} 2(1+h - y_i(K\\alpha)_i)(-y_iK_i) = \\frac{-y_iK_i(1 + h - y_i(K\\alpha)_i)}{2h}$ \n",
    "\n",
    "Finally, we solve for $\\frac{\\partial}{\\partial \\alpha} 1 - y_i(K\\alpha)_i = -y_i K_i$  \n",
    "\n",
    "Now, solving for $h'(\\alpha)$:  \n",
    "$h'(\\alpha) = \\frac{\\partial}{\\partial \\alpha} \\lambda \\alpha^T K \\alpha$  \n",
    "We can pull out $\\lambda$ as it is a constant with respect to $\\alpha$:  \n",
    "$h'(\\beta) = \\lambda \\frac{\\partial}{\\partial \\alpha} \\alpha^T K \\alpha$  \n",
    "Applying the formula $\\frac{\\partial}{\\partial x} x^TAx = (A + A^T)x$ with $A = K$ and $x = \\alpha$ we get:  \n",
    "$h'(\\alpha) = \\lambda (K + K^T) \\alpha$  \n",
    "\n",
    "We can combine these terms and reach the final gradient:\n",
    "$\\frac{1}{n} \\sum_{i=1}^{n} l'_{hh}(y_i, K\\alpha)_i) + \\lambda (K + K^T) \\alpha$ with the following cases:  \n",
    "\n",
    "$\\begin{equation}\n",
    "l'_{hh}(y_i, (K\\alpha)_i) := \\begin{cases}\n",
    "    0 \\text{  if  } y_i(K\\alpha)_i > 1 + h \\\\  \n",
    "    \\\\\n",
    "    \\frac{-y_iK_i(1 + h - y_i(K\\alpha)_i)}{2h} \\text{  if}   |1 - y_i(K\\alpha)_i| \\leq h \\\\ \n",
    "    \\\\\n",
    "    -y_i K_i \\text{  if  } y_i(K\\alpha)_i < 1 - h  \n",
    "    \\end{cases}\n",
    "\\end{equation}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a function _computegram_ that computes, for any set of datapoints $x_1, . . . , x_n$, the kernel matrix K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePolynomialKernel(x, y, b, power):\n",
    "    return (x.T.dot(y) + b)** power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computegram(X, type, b = 1, power = 1, bandwidth = 1):\n",
    "    K = np.zeros((len(X), len(X)))\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X)):\n",
    "            if(type == \"polynomial\"):\n",
    "                K[i,j] = computePolynomialKernel(X[i], X[j], b, power)\n",
    "            else:\n",
    "                K[i,j] = computeGaussianKernel(X[i], X[j], bandwidth)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a function _kerneleval_ that computes, for any set of datapoints $x_1, . . . , x_n$ and a new datapoint $x^*$, the vector of kernel evaluations $[k(x_1,x^*), ..., k(x_n, x^*)]^T$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kerneleval(X, xStar, b, power):\n",
    "    K = np.zeros((len(X), 1))\n",
    "    for i in range(len(X)):\n",
    "        K[i] = computePolynomialKernel(X[i], xStar, b, power)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider the `Digits` dataset (http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html). Download and standardize the data, if you have not done so already**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the digits X data: (1797, 64)\n",
      "Shape of the digits y data: (1797, 1)\n"
     ]
    }
   ],
   "source": [
    "# download the data\n",
    "XDigits, yDigits = load_digits(return_X_y = True)\n",
    "yDigits = yDigits.reshape((len(yDigits), 1))\n",
    "print(\"Shape of the digits X data:\", XDigits.shape)\n",
    "print(\"Shape of the digits y data:\", yDigits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize X data\n",
    "scaler = preprocessing.StandardScaler().fit(XDigits)\n",
    "XDigits = scaler.transform(XDigits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data down to 2 classes\n",
    "X_digits_train = np.zeros((360, XDigits.shape[1]))\n",
    "y_digits_train = np.zeros((360, 1))\n",
    "\n",
    "index = 0\n",
    "count = 0\n",
    "for i in range(0, yDigits.shape[0]):\n",
    "    if(yDigits[i] == 4 or yDigits[i] == 7):\n",
    "        count +=1\n",
    "        X_digits_train[index, :] = XDigits[i].T\n",
    "        y_digits_train[index, :] = yDigits[i]\n",
    "        index += 1\n",
    "\n",
    "# standardize y values to machine learning standard        \n",
    "y_digits_train[y_digits_train == 4] = -1\n",
    "y_digits_train[y_digits_train == 7] = 1\n",
    "\n",
    "X_digits_train, X_digits_test, y_digits_train, y_digits_test = train_test_split(X_digits_train, y_digits_train, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a function _mysvm_ that implements the fast gradient algorithm to train the kernel support vector machine with the smoothed hinge loss. The function takes as input the initial step-size value for the backtracking rule and a stopping criterion based on the norm of the gradient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computegrad(K, y, alpha, lam, h):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - beta: Vector to be optimized\n",
    "    - K: Gram matrix consisting of evaluations of the kernel k(x_i, x_j) for i,j=1,...,n\n",
    "    - y: Labels y_1,...,y_n corresponding to x_1,...,x_n\n",
    "    - lam: Penalty parameter lambda\n",
    "    Output:\n",
    "    - Value of the gradient at beta\n",
    "    \"\"\"\n",
    "    n = len(K)\n",
    "    summation = 0\n",
    "    for i in range(0, n):\n",
    "        iTerm = y[i]*(K[i].dot(alpha))\n",
    "        if iTerm < 1 - h:\n",
    "            summation += -y[i]*K[i]\n",
    "        elif(iTerm > 1 + h):\n",
    "            summation += 0\n",
    "        else:\n",
    "            summation += (-y[i]*K[i]*(1 + h - y[i]*K[i].dot(alpha)))/(2*h)\n",
    "    summation = summation.reshape(summation.shape[0], 1)\n",
    "    return ((1/n) * summation + (lam * (K + K.T).dot(alpha)).reshape(summation.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeobj(K, y, alpha, lam, h):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - beta: Vector to be optimized\n",
    "    - K: Gram matrix consisting of evaluations of the kernel k(x_i, x_j) for i,j=1,...,n\n",
    "    - y: Labels y_1,...,y_n corresponding to x_1,...,x_n\n",
    "    - lam: Penalty parameter lambda\n",
    "    Output:\n",
    "    - Value of the objective function at beta\n",
    "    \"\"\"\n",
    "    n = len(K)\n",
    "    summation = 0\n",
    "    Kalpha = K.dot(alpha)\n",
    "    for i in range(0, n):\n",
    "        ka_i = Kalpha[i]\n",
    "        yt = y[i] * (ka_i)\n",
    "        sumTerm = 0\n",
    "        if(yt > 1 + h):\n",
    "            sumTerm = 0\n",
    "        elif(yt < 1 - h):\n",
    "            sumTerm = 1 - yt\n",
    "        else:\n",
    "            sumTerm = (1 + h - yt)**2/(4*h)\n",
    "        summation += sumTerm\n",
    "    return (((1/n) * summation) + lam * alpha.T.dot(K).dot(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysvm(K, y, alpha, h, lamb, stepSize, maxIter, targetAccuracy):\n",
    "    theta = np.zeros((K.shape[1], 1))\n",
    "    objs = [computeobj(K, y, alpha, lamb, h)]\n",
    "    alphas = [alpha]\n",
    "    grad = computegrad(K, y, alpha, lamb, h)\n",
    "    t = 0\n",
    "    while (np.linalg.norm(grad) > targetAccuracy and t < maxIter):\n",
    "        stepSize = backtracking(K, y, alpha, stepSize, lamb, h)\n",
    "        alphaOld = copy.copy(alpha)\n",
    "        alpha = theta - stepSize * computegrad(K, y, alpha, lamb, h)\n",
    "        theta = alpha + (t/(t+3))*(alpha - alphaOld)\n",
    "        alphas.append(alpha) # saves current beta values\n",
    "        obj = computeobj(K, y, alpha, lamb, h)\n",
    "        objs.append(obj) # saves current objective value\n",
    "        grad = computegrad(K, y, alpha, lamb, h)\n",
    "        t += 1\n",
    "    return np.array(alphas), np.array(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(K, y, beta, eta, lamb, h, alpha = 0.5, gamma=0.8, max_iter=500):\n",
    "    \"\"\"\n",
    "    Implements backtracking rule\n",
    "    Inputs:\n",
    "        - X: matrix of X values\n",
    "        - y: vector of associated outcomes\n",
    "        - beta: vector of beta constants\n",
    "        - eta: initial step size\n",
    "        - rho: value of rho-logistic loss parameter\n",
    "        - lambda: scalar multiplicative factor for regularization penalty\n",
    "        - alpha: constant used to define sufficinet decrease condition, default set to 0.5\n",
    "        - gamma: constant to scale step size by until condition met, default set to 0.8\n",
    "    Outputs:\n",
    "        - step size\n",
    "    \"\"\"\n",
    "    grad = computegrad(K, y, beta, lamb, h)  # calculates the gradient at current beta\n",
    "    conditionMet = False # tracks when we find the backtracked step size\n",
    "    iter = 0\n",
    "    while iter < max_iter and not conditionMet: \n",
    "        if computeobj(K, y, beta - eta*grad, lamb, h) < (computeobj(K, y, beta, lamb, h) - alpha*eta*np.linalg.norm(grad)**2):\n",
    "            conditionMet = True\n",
    "        else:\n",
    "            eta = eta * gamma \n",
    "        iter += 1\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train your kernel support vector machine with the smoothed hinge loss and the polynomial kernel of order 7 on the the `Digits` dataset, tuning the regularization parameter λ using cross-validation. The *p*th order polynomial kernel is given by $k(x, y) = (x^Ty+b)^P$. You may take b = 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda found via cross validation: 1e-05\n"
     ]
    }
   ],
   "source": [
    "#set constants\n",
    "b = 1\n",
    "power = 7\n",
    "h = 0.5\n",
    "eta = 1 / scipy.linalg.eigh(2 / len(K) * np.dot(K, K) + 2 * lamb * K, eigvals=(len(K) - 1, len(K) - 1),\n",
    "                                 eigvals_only=True)[0]\n",
    "maxIter = 2500\n",
    "epsilon = 0.0001\n",
    "initialPoint = np.zeros((len(X_digits_train), 1))\n",
    "\n",
    "# cross validation to tune lambda\n",
    "svc = SVC(C = 1, kernel = \"poly\", degree=7, coef0 = b, gamma = 1, tol = epsilon, max_iter=maxIter)\n",
    "C = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = {'C': C}\n",
    "clf = GridSearchCV(svc, param_grid, cv=5)\n",
    "clf = clf.fit(X_digits_train, y_digits_train)\n",
    "# lamb = 1/ (len(K) *clf.best_params_[\"C\"]) ask about this!!!!!!!\n",
    "lamb = clf.best_params_[\"C\"]\n",
    "print(\"Lambda found via cross validation:\", lamb)\n",
    "\n",
    "#calculate Kernel matrix\n",
    "K = computegram(X_digits_train, \"polynomial\", b, power)\n",
    "\n",
    "alphas, objs = mysvm(K, y_digits_train, initialPoint, h, lamb, eta, maxIter, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(objs.reshape(-1, 1), color = \"blue\")\n",
    "plt.ylabel(\"Objective values\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.title(\"Objective Value vs Iteration Counter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the performance of kernel SVMs with different kernels (polynomial kernels with different orders, Gaussian RBF with different bandwidths, etc.).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGaussianKernel(x, y, bandwidth):\n",
    "    return np.exp(-(1/(2*(bandwidth**2))) * np.linalg.norm(x - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-706b2833a4d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mK_RBF_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputegram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_digits_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"RBF\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mK_RBF_1_alphas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK_RBF_1_objs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmysvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK_RBF_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_digits_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_digits_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mK_RBF_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputegram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_digits_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"RBF\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbandwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-74e85590bf0a>\u001b[0m in \u001b[0;36mmysvm\u001b[1;34m(K, y, alpha, h, lamb, stepSize, maxIter, targetAccuracy)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# saves current objective value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputegrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-410f41405639>\u001b[0m in \u001b[0;36mcomputegrad\u001b[1;34m(K, y, alpha, lam, h)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0msummation\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0msummation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msummation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msummation\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlam\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "maxIter = 1000\n",
    "eta = 1\n",
    "K_poly_9 = computegram(X_digits_train, \"polynomial\", b=b, power=9)\n",
    "K_poly_9_alphas, K_poly_9_objs = mysvm(K_poly_9, y_digits_train, np.zeros((len(X_digits_train), 1)), h, lamb, eta, maxIter, epsilon)\n",
    "\n",
    "K_poly_1 = computegram(X_digits_train, \"polynomial\", b=b, power=1)\n",
    "K_poly_1_alphas, K_poly_1_objs = mysvm(K_poly_1, y_digits_train, np.zeros((len(X_digits_train), 1)), h, lamb, eta, 10, epsilon)\n",
    "\n",
    "K_poly_3 = computegram(X_digits_train, \"polynomial\", b=b, power=3)\n",
    "K_poly_3_alphas, K_poly_3_objs = mysvm(K_poly_3, y_digits_train, np.zeros((len(X_digits_train), 1)), h, lamb, eta, maxIter, epsilon)\n",
    "\n",
    "K_poly_5 = computegram(X_digits_train, \"polynomial\", b=b, power=5)\n",
    "K_poly_5_alphas, K_poly_5_objs = mysvm(K_poly_5, y_digits_train, np.zeros((len(X_digits_train), 1)), h, lamb, eta, maxIter, epsilon)\n",
    "\n",
    "K_RBF_1 = computegram(X_digits_train, \"RBF\", bandwidth=1)\n",
    "K_RBF_1_alphas, K_RBF_1_objs = mysvm(K_RBF_1, y_digits_train, np.zeros((len(X_digits_train), 1)), h, lamb, eta, maxIter, epsilon)\n",
    "\n",
    "K_RBF_3 = computegram(X_digits_train, \"RBF\", bandwidth=3)\n",
    "K_RBF_3_alphas, K_RBF_3_objs = mysvm(K_RBF_3, y_digits_train, np.zeros((len(X_digits_train), 1)), h, lamb, eta, maxIter, epsilon)\n",
    "\n",
    "K_RBF_5 = computegram(X_digits_train, \"RBF\", bandwidth=5)\n",
    "K_RBF_5_alphas, K_RBF_5_objs = mysvm(K_RBF_5, y_digits_train, np.zeros((len(X_digits_train), 1)), h, lamb, eta, maxIter, epsilon)\n",
    "\n",
    "K_RBF_7 = computegram(X_digits_train, \"RBF\", bandwidth=7)\n",
    "K_RBF_7_alphas, K_RBF_7_objs = mysvm(K_RBF_7, y_digits_train, np.zeros((len(X_digits_train), 1)), h, lamb, eta, maxIter, epsilon)\n",
    "\n",
    "K_RBF_9 = computegram(X_digits_train, \"RBF\", bandwidth=9)\n",
    "K_RBF_9_alphas, K_RBF_9_objs = mysvm(K_RBF_9, y_digits_train, np.zeros((len(X_digits_train), 1)), h, lamb, eta, maxIter, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K_poly_1_objs.reshape(-1, 1), color = \"red\")\n",
    "plt.plot(K_poly_3_objs.reshape(-1, 1), color = \"green\")\n",
    "plt.plot(K_poly_5_objs.reshape(-1, 1), color = \"orange\")\n",
    "plt.plot(objs.reshape(-1, 1), color = \"blue\")\n",
    "plt.plot(K_poly_9_objs.reshape(-1, 1), color = \"purple\")\n",
    "\n",
    "plt.plot(K_RBF_1_objs.reshape(-1, 1), color = \"yellow\")\n",
    "plt.plot(K_RBF_3_objs.reshape(-1, 1), color = \"pink\")\n",
    "plt.plot(K_RBF_5_objs.reshape(-1, 1), color = \"gray\")\n",
    "plt.plot(K_RBF_7_objs.reshape(-1, 1), color = \"black\")\n",
    "plt.plot(K_RBF_9_objs.reshape(-1, 1), color = \"brown\")\n",
    "\n",
    "plt.ylabel(\"Objective values\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.title(\"Objective Value vs Iteration Counter\")\n",
    "plt.legend([\"Poly 1\", \"Poly 3\", \"Poly 5\", \"Poly 7\", \"Poly 9\", \"RBF 1\", \"RBF 3\", \"RBF 5\", \"RBF 7\", \"RBF 9\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification_error(alpha, X_train, X_test, y_test, type, bandwidth = 1, power = 1):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    if(type == \"polynomial\"):\n",
    "        kernel = computegram(X_train, \"polynomial\", 1, power)\n",
    "    else:\n",
    "        kernel = computegram(X_train, \"RBF\", bandwidth)\n",
    "    for i in range(0, len(X_test)):\n",
    "        if(kernel[i].dot(alpha) > 0):\n",
    "            prediction = 1\n",
    "        else:\n",
    "            prediction = -1\n",
    "        y_pred.append(prediction)\n",
    "    return np.mean(np.array(y_pred).reshape(-1, 1) != y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_poly_1_misclass = misclassification_error(K_poly_1_alphas[-1], X_digits_train, X_digits_test, y_digits_test, \"polynomial\", power=1)\n",
    "K_poly_3_misclass = misclassification_error(K_poly_3_alphas[-1], X_digits_train, X_digits_test, y_digits_test, \"polynomial\", power=3)\n",
    "K_poly_5_misclass = misclassification_error(K_poly_5_alphas[-1], X_digits_train, X_digits_test, y_digits_test, \"polynomial\", power=5)\n",
    "K_poly_7_misclass = misclassification_error(alphas[-1], X_digits_train, X_digits_test, y_digits_test, \"polynomial\", power=7)\n",
    "K_poly_9_misclass = misclassification_error(K_poly_9_alphas[-1], X_digits_train, X_digits_test, y_digits_test, \"polynomial\", power=9)\n",
    "K_RBF_1_misclass = misclassification_error(K_RBF_1_alphas[-1], X_digits_train, X_digits_test, y_digits_test, \"RBF\", bandwidth=1)\n",
    "K_RBF_3_misclass = misclassification_error(K_RBF_3_alphas[-1], X_digits_train, X_digits_test, y_digits_test, \"RBF\", bandwidth=3)\n",
    "K_RBF_5_misclass = misclassification_error(K_RBF_5_alphas[-1], X_digits_train, X_digits_test, y_digits_test, \"RBF\", bandwidth=5)\n",
    "K_RBF_7_misclass = misclassification_error(K_RBF_7_alphas[-1], X_digits_train, X_digits_test, y_digits_test, \"RBF\", bandwidth=7)\n",
    "K_RBF_9_misclass = misclassification_error(K_RBF_9_alphas[-1], X_digits_train, X_digits_test, y_digits_test, \"RBF\", bandwidth=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Power/Bandwidth 1\": (K_poly_1_misclass, K_RBF_1_misclass), \"Power/Bandwidth 3\": (K_poly_3_misclass, K_RBF_3_misclass),\n",
    "                \"Power/Bandwidth 5\": (K_poly_5_misclass, K_RBF_5_misclass), \"Power/Bandwidth 7\": (K_poly_7_misclass, K_RBF_7_misclass),\n",
    "                \"Power/Bandwidth 9\": (K_poly_9_misclass, K_RBF_9_misclass)})\n",
    "df.rename(index={0:\"Polnomial Kenel\", 1:\"RBF\"}, inplace = True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Data Competition Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this exercise, you are going to train support vector machines (SVMs) using the data competition 2 project dataset (with 100 classes). You will consider here all classes in the dataset. You may work on this exercise on your own computer first. Note, however, that you need AWS to run the experiments for this entire exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../Kaggle/data558spring2019-competition2\"\n",
    "\n",
    "X_train = np.load(os.path.join(data_dir, 'train_features.npy'))\n",
    "y_train = np.load(os.path.join(data_dir, 'train_labels.npy')).reshape(-1, 1)\n",
    "X_test = np.load(os.path.join(data_dir, 'val_features.npy'))\n",
    "y_test = np.load(os.path.join(data_dir, 'val_labels.npy')).reshape(-1, 1)\n",
    "\n",
    "# standardize X data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# standardize y labels\n",
    "# scaler = preprocessing.StandardScaler().fit(y_train)\n",
    "# y_train = scaler.transform(y_train)\n",
    "# y_test = scaler.transform(y_test)\n",
    "\n",
    "print(\"Dimension of X_train:\", X_train.shape)\n",
    "print(\"Dimension of y_train:\", y_train.shape)\n",
    "print(\"Dimension of X_test:\", X_test.shape)\n",
    "print(\"Dimension of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In a one-vs-one fashion, for each pair of classes, train a linear SVM classifier using scikitlearn’s function `LinearSVC`, with the default value for the regularization parameter. Compute the *multi-class misclassification error* obtained using these classifiers trained\n",
    "in a one-vs-one fashion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(label1, label2):\n",
    "    \"\"\"\n",
    "    Gets training and test data from kaggle dataset filtered by 2 specified labels\n",
    "    Inputs:\n",
    "        - label1: first label for filtering kaggle image dataset (0 - 9)\n",
    "        - label2: second label for filtering kaggle image dataset (0 - 9), must be different than label1\n",
    "    Outputs:\n",
    "        - X_train: 75% of filtered x values to use for model training\n",
    "        - X_test: 25% of filtered x values to use for model testing\n",
    "        - y_train: 75% of filtered y values to use for model training\n",
    "        - y_test: 25% of filtered y values to use for model testing\n",
    "    \"\"\"\n",
    "    #filters x training values down to rows with matching input labels\n",
    "    x_train_filtered = np.zeros((1000, X_train.shape[1]))\n",
    "    y_train_filtered = np.zeros((1000, 1))\n",
    "    index = 0\n",
    "    for i in range(y_train.shape[0]):\n",
    "        if(y_train[i] == label1 or y_train[i] == label2):\n",
    "            x_train_filtered[index, :] = X_train[i].T\n",
    "            y_train_filtered[index, :] = y_train[i]\n",
    "            index += 1\n",
    "    \n",
    "    # reset train y data to -1 and 1 to fit machine learning standards\n",
    "    y_train_filtered[y_train_filtered == label1] = -1\n",
    "    y_train_filtered[y_train_filtered == label2] = 1\n",
    "\n",
    "    #filters x testing values down to rows with matching input labels\n",
    "    x_test_filtered = np.zeros((200, X_test.shape[1]))\n",
    "    y_test_filtered = np.zeros((200, 1))\n",
    "    index = 0\n",
    "    for i in range(y_test.shape[0]):\n",
    "        if(y_test[i] == label1 or y_test[i] == label1):\n",
    "            x_test_filtered[index, :] = X_test[i].T\n",
    "            y_test_filtered[index, :] = y_test[i]\n",
    "            index += 1\n",
    "\n",
    "    # reset y data to -1 and 1 to fit machine learning standards\n",
    "    y_test_filtered[y_test_filtered == label1] = -1\n",
    "    y_test_filtered[y_test_filtered == label2] = 1\n",
    "\n",
    "    # standardize x values\n",
    "    xScaler = preprocessing.StandardScaler()\n",
    "    X_train_filtered = xScaler.fit_transform(x_train_filtered)\n",
    "    X_test_filtered = xScaler.transform(x_test_filtered)\n",
    "    \n",
    "    return X_train_filtered, X_test_filtered, y_train_filtered, y_test_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def oneVsOneSVM(X, y):\n",
    "    misclassificationErrors = []\n",
    "    for i in range(0,100):\n",
    "        for j in range(i + 1, 100):\n",
    "            X_train_OneVsOne, X_test_OneVsOne, y_train_OneVsOne, y_test_OneVsOne = getData(i, j)\n",
    "            clf = LinearSVC(random_state=0, tol=1)\n",
    "            error = clf.fit(X_train_OneVsOne, y_train_OneVsOne).score(X_test_OneVsOne, y_test_OneVsOne)\n",
    "            misclassificationErrors.append(error)\n",
    "            print(error)\n",
    "    return misclassificationErrors\n",
    "\n",
    "misclassificationErrorsOneVsOne = oneVsOneSVM(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In a one-vs-rest fashion, for each class, train a linear SVM classifier using scikit-learn’s function `LinearSVC`, with the default value for $\\lambda_c$. Compute the multi-class misclassification error obtained using these classifiers trained in a one-vs-rest fashion.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVsRestSVM(X, y):\n",
    "    misclassificationErrors = []\n",
    "    for label in np.unique(y):\n",
    "        print(label)\n",
    "        y_temp = copy.deepcopy(y)\n",
    "        y_temp[y_temp == label] = -1\n",
    "        y_temp[y_temp != -1] = 1\n",
    "        clf = LinearSVC(random_state=0, tol=1)\n",
    "        error = clf.fit(X, y_temp).score(X_test, y_test)\n",
    "        print(error)\n",
    "        misclassificationErrors.append(error)\n",
    "    return misclassificationErrors\n",
    "        \n",
    "misclassificationErrorsOneVsRest = oneVsRestSVM(X_train, y_train)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Redo all questions above now using your own code for the linear SVMs from Exercise 1. Make to sure to run preliminary experiments to decide how to set the stopping criterion to a value that allows the experiments to complete in a reasonable amount of time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
