{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Homework 2 </center>\n",
    "<center> Tara Wilson </center>\n",
    "<center> DATA 558 </center>\n",
    "<center> April 18, 2019 </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement in Python a first version of your own fast gradient algorithm to solve the $l_2^2$-regularized logistic regression problem. Recall from the lectures that the logistic regression problem writes as $$min F(\\beta) := \\frac{1}{n} \\sum_{i=1}^{n} \\log(1 + \\exp(-y_ix_i^T\\beta)) + \\lambda ||\\beta||_2^2$$. We use here the machine learning convention for the labels that is $y_i \\in {-1, 1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Fast Gradient\n",
    "**The  fast gradient algorithm is outlined in Algorithm 1. The algorithm requires a subroutine\n",
    "that computes the gradient for any β.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assume that _d = 1_ and _n = 1_. The sample is then of size 1 and boils down to just _(x, y)_. The function *F* writes simply as $$F(\\beta) = \\log(1+\\exp(-yx\\beta))+ \\lambda\\beta^2$$  Compute and write down the gradient $\\nabla F$ of F.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assume now that _d > 1_ and _n > 1_. Using the previous result and the linearity of differentiation, compute and write down the gradient $\\nabla F$ of F.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider the Spam dataset from The Elements of Statistical Learning (You can get it here: https://web.stanford.edu/~hastie/ElemStatLearn/). Standardize the data (i.e., center the features and divide them by their standard deviation, and also change the output labels to +/- 1).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv(\"https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data\", sep=\" \", \n",
    "                   na_values=\"?\", header=None)\n",
    "spam = spam.dropna()\n",
    "X = spam.iloc[:, 0:57]\n",
    "y = spam.iloc[:,-1].replace(0,-1)\n",
    "y = np.array(y).reshape(y.shape[0], 1)\n",
    "\n",
    "# Divide the data into training and test sets. By default, 25% goes into the test set.\n",
    "X, X_test, y, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "\n",
    "# standardizing X by subtracting the mean of the predictors and dividing by their standard deviation\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a function _computegrad_ that computes and returns $\\nabla F(\\beta)$ for any $\\beta$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computegrad(X, y, beta, lamb = 0.05):\n",
    "    \"\"\"\n",
    "    Computes the gradient for the fast gradient algorithm\n",
    "    Inputs:\n",
    "        - X: matrix of X values\n",
    "        - y: vector of associated outcomes\n",
    "        - beta: vector of beta constants\n",
    "        - lambda: scalar multiplicative factor for regularization penalty (optional, defaults to 0.05)\n",
    "    Outputs:\n",
    "        - vector gradient for passed in parameters\n",
    "    \"\"\"\n",
    "    n = len(X) \n",
    "    P = np.zeros((X.shape[0], X.shape[0]))\n",
    "    for i in range(0, X.shape[0]):\n",
    "        xi = X[i,:]\n",
    "        yi = y[i][0]\n",
    "        #yi = y[i]\n",
    "        p = ((np.exp(-yi * (xi).dot(beta))) / (1 + (np.exp(-yi * (xi).dot(beta)))))\n",
    "        P[i,i] = p  \n",
    "    return ((-1/n) * X.T.dot(P).dot(y) + 2*lamb*beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeobj(X, y, beta, lamb = 0.05):\n",
    "    \"\"\"\n",
    "    Computes the objective for ridge regression problem\n",
    "    Inputs:\n",
    "        - X: matrix of X values\n",
    "        - y: vector of associated outcomes\n",
    "        - beta: vector of beta constants\n",
    "        - lambda: scalar multiplicative factor for regularization penalty (optional, defaults to 0.05)\n",
    "    Outputs:\n",
    "        - objective for passed in parameters\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    summation = 0\n",
    "    for i in range(0, X.shape[0]):\n",
    "        xi = X[i,:]\n",
    "        #yi = y[i][0]\n",
    "        yi = y[i]\n",
    "        x = -yi*xi.dot(beta)\n",
    "        if(x > 0):\n",
    "            a = x + 1\n",
    "            logTerm = a + np.log(np.exp(-a) + np.exp(x-a))\n",
    "        else:\n",
    "            logTerm = np.log(1 + np.exp(x))\n",
    "        summation = summation + logTerm\n",
    "    \n",
    "    return ((1/n) * summation + (lamb * np.sum(beta**2)))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a function _backtracking_ that implements the backtracking rule.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(X, y, beta, eta, alpha = 0.5, gamma=0.8):\n",
    "    \"\"\"\n",
    "    Implements backtracking rule\n",
    "    Inputs:\n",
    "        - X: matrix of X values\n",
    "        - y: vector of associated outcomes\n",
    "        - beta: vector of beta constants\n",
    "        - eta: initial step size\n",
    "        - alpha: constant used to define sufficinet decrease condition\n",
    "        - gamma: constant to scale step size by until condition met         \n",
    "    Outputs:\n",
    "        - step size\n",
    "    \"\"\"\n",
    "    grad = computegrad(X, y, beta)  # Gradient at beta\n",
    "    conditionMet = False\n",
    "    while not conditionMet:    \n",
    "        if computeobj(X, y, beta - eta*grad)< computeobj(X, y, beta) - alpha*eta*np.linalg.norm(grad)**2:\n",
    "            conditionMet = True\n",
    "        else:\n",
    "            eta = eta * gamma            \n",
    "    return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a function _graddescent_ that implements the gradient descent algorithm with the backtracking rule to tune the step-size. The function _graddescent_ calls _computegrad_ and _backtracking_ as subroutines. The function takes as input the initial point, the initial step-size value, and the target accuracy ε. The stopping criterion is $||\\nabla F|| \\leq \\epsilon$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graddescent(X, y, stepSize, targetAccuracy = 0.001, lamb = 0.05):\n",
    "    \"\"\"\n",
    "    Implements gradient descent algorithm with backtracking\n",
    "    Inputs:\n",
    "        - X: matrix of X values\n",
    "        - y: vector of associated outcomes\n",
    "        - stepSize: initial step size\n",
    "        - targetAccuracy: target accuracy value for algorithm\n",
    "        - lambda: scalar multiplicative factor for regularization penalty (optional, defaults to 0.05)\n",
    "    Outputs:\n",
    "        - betas: vector of improved betas after final iteration\n",
    "        - objs: vector of objective values for each iteration\n",
    "    \"\"\"\n",
    "    x = X\n",
    "    beta = np.zeros((X.shape[1], 1))\n",
    "    objs = [computeobj(x, y, beta, lamb)]\n",
    "    betas = [beta]\n",
    "    grad = computegrad(x, y, beta, lamb)\n",
    "    t = 0\n",
    "    while (np.linalg.norm(grad) > targetAccuracy):\n",
    "        stepSize = backtracking(x, y, beta, stepSize)\n",
    "        beta = beta - stepSize * grad\n",
    "        betas.append(beta) # saves current beta values\n",
    "        obj = computeobj(x, y, beta, lamb)\n",
    "        objs.append(obj) # saves current objective value\n",
    "        grad = computegrad(x, y, beta, lamb)\n",
    "        t = t + 1\n",
    "    return np.array(betas), np.array(objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a function _fastgradalgo_ that implements the fast gradient algorithm described in Algorithm 1. The function _fastgradalgo_ calls _computegrad_ and _backtracking_ as subroutines. The function takes as input the initial step-size value for the backtracking rule and the target accuracy ε. The stopping criterion is $||\\nabla F|| \\leq \\epsilon$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastgradalgo(X, y, stepSize, targetAccuracy = 0.001, lamb = 0.05):\n",
    "    \"\"\"\n",
    "    Implements fast gradient descent algorithm with backtracking\n",
    "    Inputs:\n",
    "        - X: matrix of X values\n",
    "        - y: vector of associated outcomes\n",
    "        - stepSize: initial step size\n",
    "        - targetAccuracy: target accuracy value for algorithm\n",
    "        - lambda: scalar multiplicative factor for regularization penalty (optional, defaults to 0.05)\n",
    "    Outputs:\n",
    "        - betas: vector of improved betas after final iteration\n",
    "        - objs: vector of objective values for each iteration\n",
    "    \"\"\"\n",
    "    x = X\n",
    "    beta = np.zeros((X.shape[1], 1))\n",
    "    theta = np.zeros((X.shape[1], 1))\n",
    "    objs = [computeobj(x, y, beta, lamb)]\n",
    "    betas = [beta]\n",
    "    #thetas = [theta]\n",
    "    grad = computegrad(x, y, theta, lamb)\n",
    "    t = 0 \n",
    "    while (np.linalg.norm(grad) > targetAccuracy):\n",
    "        stepSize = backtracking(x, y, beta, stepSize)\n",
    "        betaOld = beta\n",
    "        beta = theta - stepSize * computegrad(x, y, theta, lamb)\n",
    "        theta = beta + (t/(t+3))*(beta - betaOld)\n",
    "        betas.append(beta) # saves current beta values\n",
    "        obj = computeobj(x, y, beta, lamb)\n",
    "        objs.append(obj) # saves current objective value\n",
    "        grad = computegrad(x, y, theta, lamb)\n",
    "        #thetas.append(theta)\n",
    "        t = t + 1\n",
    "    return np.array(betas), np.array(objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the estimate described in the course to initialize the step-size. Set the target accuracy to $\\epsilon = 10^{-4}$. Run _graddescent_ and _fastgradalgo_ on the training set of the Spam dataset for λ = 0.1. Plot the curve of the objective values $F(\\beta_t)$ for both algorithms versus the iteration counter t (use different colors). What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set constants\n",
    "epsilon = 0.0001\n",
    "lamb = 0.1\n",
    "\n",
    "# calculate initial step size\n",
    "n = len(X)\n",
    "eq = (1/n * X.T.dot(X))\n",
    "eigVals = eigh(eq)[0]\n",
    "stepSize = 1 / (max(eigVals) + lamb)\n",
    "\n",
    "# run algorithms\n",
    "gdBeta, gdObjs = graddescent(X, y, stepSize = stepSize, targetAccuracy = epsilon, lamb = lamb)\n",
    "fgBeta, fgObjs = fastgradalgo(X, y, stepSize = stepSize, targetAccuracy = epsilon, lamb = lamb)\n",
    "\n",
    "# plot \n",
    "plt.plot(gdObjs, color = \"red\")\n",
    "plt.plot(fgObjs, color = \"purple\")\n",
    "plt.ylabel(\"Objective values\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.title(\"Objective Value vs Iteration Counter\")\n",
    "plt.legend([\"Gradient Descent\", \"Fast Gradient Algorithm\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Denote by $\\beta_T$ the final iterate of your fast gradient algorithm. Compare $\\beta_T$ to the $\\beta^*$ found by _scikit-learn_. Compare the objective value for $\\beta_T$ to the one for $\\beta^*$. What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beta_T</th>\n",
       "      <th>Beta*</th>\n",
       "      <th>Difference Beta_T - Beta*</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0296838835539165]</td>\n",
       "      <td>[-0.04469692253313419]</td>\n",
       "      <td>[0.0743808060870507]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.022269283777930245]</td>\n",
       "      <td>[-0.11361059504569675]</td>\n",
       "      <td>[0.0913413112677665]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0866739372015923]</td>\n",
       "      <td>[0.09690999446359508]</td>\n",
       "      <td>[-0.010236057262002773]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.058816618172809795]</td>\n",
       "      <td>[0.4875582765158724]</td>\n",
       "      <td>[-0.4287416583430626]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.17540317127471478]</td>\n",
       "      <td>[0.43779879832206053]</td>\n",
       "      <td>[-0.26239562704734576]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.12131126324429015]</td>\n",
       "      <td>[0.13892748281422265]</td>\n",
       "      <td>[-0.017616219569932506]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.2946632634166697]</td>\n",
       "      <td>[1.4385463309899271]</td>\n",
       "      <td>[-1.1438830675732574]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.13224707928458088]</td>\n",
       "      <td>[0.2245899417875963]</td>\n",
       "      <td>[-0.09234286250301543]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.12260681873269294]</td>\n",
       "      <td>[0.17398688453783778]</td>\n",
       "      <td>[-0.05138006580514484]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.05079485625763609]</td>\n",
       "      <td>[0.03227530908493829]</td>\n",
       "      <td>[0.018519547172697805]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.13950255325064911]</td>\n",
       "      <td>[0.04387509862007601]</td>\n",
       "      <td>[0.0956274546305731]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[-0.038847048537675004]</td>\n",
       "      <td>[-0.10934868071217294]</td>\n",
       "      <td>[0.07050163217449792]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.05655071903590183]</td>\n",
       "      <td>[-0.018811760590307]</td>\n",
       "      <td>[0.07536247962620883]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.032674212099099126]</td>\n",
       "      <td>[0.028063150376995444]</td>\n",
       "      <td>[0.004611061722103682]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.10915881669819939]</td>\n",
       "      <td>[0.45037305357333285]</td>\n",
       "      <td>[-0.34121423687513347]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.1974573751332206]</td>\n",
       "      <td>[0.4656425114471866]</td>\n",
       "      <td>[-0.268185136313966]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.15749808321649364]</td>\n",
       "      <td>[0.36099353544749735]</td>\n",
       "      <td>[-0.2034954522310037]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.11932576525352635]</td>\n",
       "      <td>[0.0938879291022867]</td>\n",
       "      <td>[0.02543783615123965]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.11493903134424836]</td>\n",
       "      <td>[0.2139460455628431]</td>\n",
       "      <td>[-0.09900701421859474]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.1291975717737287]</td>\n",
       "      <td>[0.7831213932330361]</td>\n",
       "      <td>[-0.6539238214593074]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.2358194019246865]</td>\n",
       "      <td>[0.29627788420619505]</td>\n",
       "      <td>[-0.060458482281508547]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.10136382891803303]</td>\n",
       "      <td>[0.5502578686710242]</td>\n",
       "      <td>[-0.4488940397529912]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.2502125128920384]</td>\n",
       "      <td>[1.4277177017046487]</td>\n",
       "      <td>[-1.1775051888126105]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.15321801936541665]</td>\n",
       "      <td>[0.3451168308186003]</td>\n",
       "      <td>[-0.19189881145318363]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[-0.14765055502447363]</td>\n",
       "      <td>[-1.1301395438186563]</td>\n",
       "      <td>[0.9824889887941827]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[-0.10449256930957569]</td>\n",
       "      <td>[-0.4293126930146473]</td>\n",
       "      <td>[0.32482012370507163]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[-0.12074601166273666]</td>\n",
       "      <td>[-0.860151083388336]</td>\n",
       "      <td>[0.7394050717255993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[-0.03509294427953719]</td>\n",
       "      <td>[0.1866795558535291]</td>\n",
       "      <td>[-0.2217725001330663]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[-0.05008703345454126]</td>\n",
       "      <td>[-0.19237112522248148]</td>\n",
       "      <td>[0.1422840917679402]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[-0.07532263890928904]</td>\n",
       "      <td>[-0.19829282182847535]</td>\n",
       "      <td>[0.12297018291918631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[-0.0326612207770715]</td>\n",
       "      <td>[-0.11030507891778182]</td>\n",
       "      <td>[0.07764385814071031]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[-0.01349971061471965]</td>\n",
       "      <td>[0.07715786546614722]</td>\n",
       "      <td>[-0.09065757608086687]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[-0.07907973557414487]</td>\n",
       "      <td>[-0.4025442623019846]</td>\n",
       "      <td>[0.3234645267278397]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[-0.01606713786635855]</td>\n",
       "      <td>[-0.043243001760747184]</td>\n",
       "      <td>[0.027175863894388634]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[-0.05663011077202423]</td>\n",
       "      <td>[-0.22270551320810061]</td>\n",
       "      <td>[0.16607540243607638]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[-0.004136527384516195]</td>\n",
       "      <td>[0.2878674430021392]</td>\n",
       "      <td>[-0.2920039703866554]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[-0.08060187947411203]</td>\n",
       "      <td>[-0.08433342882081668]</td>\n",
       "      <td>[0.0037315493467046518]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[-0.022386096990509]</td>\n",
       "      <td>[-0.0017099851749627917]</td>\n",
       "      <td>[-0.020676111815546207]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[-0.057944012180361465]</td>\n",
       "      <td>[-0.17919558003657693]</td>\n",
       "      <td>[0.12125156785621546]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[0.015505646415270935]</td>\n",
       "      <td>[-0.06325650634495451]</td>\n",
       "      <td>[0.07876215276022545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[-0.05250048990108763]</td>\n",
       "      <td>[-0.1907972647549831]</td>\n",
       "      <td>[0.13829677485389547]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[-0.09656190968972177]</td>\n",
       "      <td>[-0.6100560017134963]</td>\n",
       "      <td>[0.5134940920237745]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[-0.0587154639885505]</td>\n",
       "      <td>[-0.0972321702121846]</td>\n",
       "      <td>[0.038516706223634095]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[-0.06652963087808476]</td>\n",
       "      <td>[-0.35285702418589554]</td>\n",
       "      <td>[0.28632739330781076]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[-0.1108656641562263]</td>\n",
       "      <td>[-0.5501798013330907]</td>\n",
       "      <td>[0.43931413717686446]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[-0.11036118234815336]</td>\n",
       "      <td>[-0.6788778797445559]</td>\n",
       "      <td>[0.5685166973964025]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[-0.04533567623754505]</td>\n",
       "      <td>[-0.15210928704564974]</td>\n",
       "      <td>[0.10677361080810469]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[-0.05977851225762943]</td>\n",
       "      <td>[-0.3827224926691202]</td>\n",
       "      <td>[0.32294398041149075]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[-0.06763022982755997]</td>\n",
       "      <td>[-0.4306434762003247]</td>\n",
       "      <td>[0.3630132463727648]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[-0.03256922697908378]</td>\n",
       "      <td>[-0.07580971679058725]</td>\n",
       "      <td>[0.04324048981150347]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[-0.028081096220753707]</td>\n",
       "      <td>[-0.16133934350771542]</td>\n",
       "      <td>[0.13325824728696173]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[0.1597841258457005]</td>\n",
       "      <td>[0.3630221996818857]</td>\n",
       "      <td>[-0.2032380738361852]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>[0.25556418243240026]</td>\n",
       "      <td>[1.5817193707629016]</td>\n",
       "      <td>[-1.3261551883305014]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[0.060286061604149926]</td>\n",
       "      <td>[0.562112808387389]</td>\n",
       "      <td>[-0.501826746783239]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>[0.06690207608265676]</td>\n",
       "      <td>[0.3294284378916943]</td>\n",
       "      <td>[-0.26252636180903755]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>[0.13370429039713977]</td>\n",
       "      <td>[0.9236627234327401]</td>\n",
       "      <td>[-0.7899584330356004]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>[0.1667512146358013]</td>\n",
       "      <td>[0.4404605646913808]</td>\n",
       "      <td>[-0.2737093500555795]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Beta_T                     Beta*  \\\n",
       "0      [0.0296838835539165]    [-0.04469692253313419]   \n",
       "1   [-0.022269283777930245]    [-0.11361059504569675]   \n",
       "2      [0.0866739372015923]     [0.09690999446359508]   \n",
       "3    [0.058816618172809795]      [0.4875582765158724]   \n",
       "4     [0.17540317127471478]     [0.43779879832206053]   \n",
       "5     [0.12131126324429015]     [0.13892748281422265]   \n",
       "6      [0.2946632634166697]      [1.4385463309899271]   \n",
       "7     [0.13224707928458088]      [0.2245899417875963]   \n",
       "8     [0.12260681873269294]     [0.17398688453783778]   \n",
       "9     [0.05079485625763609]     [0.03227530908493829]   \n",
       "10    [0.13950255325064911]     [0.04387509862007601]   \n",
       "11  [-0.038847048537675004]    [-0.10934868071217294]   \n",
       "12    [0.05655071903590183]      [-0.018811760590307]   \n",
       "13   [0.032674212099099126]    [0.028063150376995444]   \n",
       "14    [0.10915881669819939]     [0.45037305357333285]   \n",
       "15     [0.1974573751332206]      [0.4656425114471866]   \n",
       "16    [0.15749808321649364]     [0.36099353544749735]   \n",
       "17    [0.11932576525352635]      [0.0938879291022867]   \n",
       "18    [0.11493903134424836]      [0.2139460455628431]   \n",
       "19     [0.1291975717737287]      [0.7831213932330361]   \n",
       "20     [0.2358194019246865]     [0.29627788420619505]   \n",
       "21    [0.10136382891803303]      [0.5502578686710242]   \n",
       "22     [0.2502125128920384]      [1.4277177017046487]   \n",
       "23    [0.15321801936541665]      [0.3451168308186003]   \n",
       "24   [-0.14765055502447363]     [-1.1301395438186563]   \n",
       "25   [-0.10449256930957569]     [-0.4293126930146473]   \n",
       "26   [-0.12074601166273666]      [-0.860151083388336]   \n",
       "27   [-0.03509294427953719]      [0.1866795558535291]   \n",
       "28   [-0.05008703345454126]    [-0.19237112522248148]   \n",
       "29   [-0.07532263890928904]    [-0.19829282182847535]   \n",
       "30    [-0.0326612207770715]    [-0.11030507891778182]   \n",
       "31   [-0.01349971061471965]     [0.07715786546614722]   \n",
       "32   [-0.07907973557414487]     [-0.4025442623019846]   \n",
       "33   [-0.01606713786635855]   [-0.043243001760747184]   \n",
       "34   [-0.05663011077202423]    [-0.22270551320810061]   \n",
       "35  [-0.004136527384516195]      [0.2878674430021392]   \n",
       "36   [-0.08060187947411203]    [-0.08433342882081668]   \n",
       "37     [-0.022386096990509]  [-0.0017099851749627917]   \n",
       "38  [-0.057944012180361465]    [-0.17919558003657693]   \n",
       "39   [0.015505646415270935]    [-0.06325650634495451]   \n",
       "40   [-0.05250048990108763]     [-0.1907972647549831]   \n",
       "41   [-0.09656190968972177]     [-0.6100560017134963]   \n",
       "42    [-0.0587154639885505]     [-0.0972321702121846]   \n",
       "43   [-0.06652963087808476]    [-0.35285702418589554]   \n",
       "44    [-0.1108656641562263]     [-0.5501798013330907]   \n",
       "45   [-0.11036118234815336]     [-0.6788778797445559]   \n",
       "46   [-0.04533567623754505]    [-0.15210928704564974]   \n",
       "47   [-0.05977851225762943]     [-0.3827224926691202]   \n",
       "48   [-0.06763022982755997]     [-0.4306434762003247]   \n",
       "49   [-0.03256922697908378]    [-0.07580971679058725]   \n",
       "50  [-0.028081096220753707]    [-0.16133934350771542]   \n",
       "51     [0.1597841258457005]      [0.3630221996818857]   \n",
       "52    [0.25556418243240026]      [1.5817193707629016]   \n",
       "53   [0.060286061604149926]       [0.562112808387389]   \n",
       "54    [0.06690207608265676]      [0.3294284378916943]   \n",
       "55    [0.13370429039713977]      [0.9236627234327401]   \n",
       "56     [0.1667512146358013]      [0.4404605646913808]   \n",
       "\n",
       "   Difference Beta_T - Beta*  \n",
       "0       [0.0743808060870507]  \n",
       "1       [0.0913413112677665]  \n",
       "2    [-0.010236057262002773]  \n",
       "3      [-0.4287416583430626]  \n",
       "4     [-0.26239562704734576]  \n",
       "5    [-0.017616219569932506]  \n",
       "6      [-1.1438830675732574]  \n",
       "7     [-0.09234286250301543]  \n",
       "8     [-0.05138006580514484]  \n",
       "9     [0.018519547172697805]  \n",
       "10      [0.0956274546305731]  \n",
       "11     [0.07050163217449792]  \n",
       "12     [0.07536247962620883]  \n",
       "13    [0.004611061722103682]  \n",
       "14    [-0.34121423687513347]  \n",
       "15      [-0.268185136313966]  \n",
       "16     [-0.2034954522310037]  \n",
       "17     [0.02543783615123965]  \n",
       "18    [-0.09900701421859474]  \n",
       "19     [-0.6539238214593074]  \n",
       "20   [-0.060458482281508547]  \n",
       "21     [-0.4488940397529912]  \n",
       "22     [-1.1775051888126105]  \n",
       "23    [-0.19189881145318363]  \n",
       "24      [0.9824889887941827]  \n",
       "25     [0.32482012370507163]  \n",
       "26      [0.7394050717255993]  \n",
       "27     [-0.2217725001330663]  \n",
       "28      [0.1422840917679402]  \n",
       "29     [0.12297018291918631]  \n",
       "30     [0.07764385814071031]  \n",
       "31    [-0.09065757608086687]  \n",
       "32      [0.3234645267278397]  \n",
       "33    [0.027175863894388634]  \n",
       "34     [0.16607540243607638]  \n",
       "35     [-0.2920039703866554]  \n",
       "36   [0.0037315493467046518]  \n",
       "37   [-0.020676111815546207]  \n",
       "38     [0.12125156785621546]  \n",
       "39     [0.07876215276022545]  \n",
       "40     [0.13829677485389547]  \n",
       "41      [0.5134940920237745]  \n",
       "42    [0.038516706223634095]  \n",
       "43     [0.28632739330781076]  \n",
       "44     [0.43931413717686446]  \n",
       "45      [0.5685166973964025]  \n",
       "46     [0.10677361080810469]  \n",
       "47     [0.32294398041149075]  \n",
       "48      [0.3630132463727648]  \n",
       "49     [0.04324048981150347]  \n",
       "50     [0.13325824728696173]  \n",
       "51     [-0.2032380738361852]  \n",
       "52     [-1.3261551883305014]  \n",
       "53      [-0.501826746783239]  \n",
       "54    [-0.26252636180903755]  \n",
       "55     [-0.7899584330356004]  \n",
       "56     [-0.2737093500555795]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "betaT = fgBeta[-1]\n",
    "\n",
    "model = LogisticRegression(penalty='l2', dual=False, C=1/lamb, tol=epsilon, fit_intercept=False, solver='saga')\n",
    "model = model.fit(X, y)\n",
    "betaStar = model.coef_\n",
    "\n",
    "betaT\n",
    "betaStar = betaStar[0].reshape(57,1)\n",
    "betaDiff = betaT - betaStar\n",
    "\n",
    "df = pd.DataFrame({\"Beta_T\":betaT.tolist(), \"Beta*\":betaStar.tolist(), \"Difference Beta_T - Beta*\":betaDiff.tolist()})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Objective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Beta*</th>\n",
       "      <td>1.722303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beta_T</th>\n",
       "      <td>0.450686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Objective\n",
       "Beta*    1.722303\n",
       "Beta_T   0.450686"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "objBetaT = fgObjs[-1]\n",
    "objBetaStar = computeobj(X, y, betaStar.reshape(57, 1), lamb = 0.1)\n",
    "\n",
    "df = pd.DataFrame([[objBetaStar], [objBetaT]], index = [\"Beta*\", \"Beta_T\"], columns=[\"Objective\"])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about this!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run cross-validation on the training set of the Spam dataset using _scikit-learn_ to find the optimal value of λ. Run _graddescent_ and _fastgradalgo_ to optimize the objective with that value of λ. Plot the curve of the objective values $F(\\beta_t)$ for both algorithms versus the iteration counter t. Plot the misclassification error on the training set for both algorithms versus the iteration counter t. Plot the misclassification error on the test set for both algorithms versus the iteration counter t. What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation to find best lambda\n",
    "param_grid = {'C': [0.001, 0.05, 0.01, .5, 0.1, .5, 1, 5, 10, 50, 100, 500, 1000] }\n",
    "clf = GridSearchCV(model, param_grid)\n",
    "newLamb = clf.fit(X, y).best_estimator_.get_params()['C']\n",
    "#score = clf.fit(X, y).best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newLamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run algorithms\n",
    "newGdBeta, newGdObjs = graddescent(X, y, stepSize = stepSize, targetAccuracy = epsilon, lamb = newLamb)\n",
    "newFgBeta, newFgObjs = fastgradalgo(X, y, stepSize = stepSize, targetAccuracy = epsilon, lamb = newLamb)\n",
    "\n",
    "# plot curves\n",
    "plt.plot(newGdObjs, color = \"red\")\n",
    "plt.plot(newFgObjs, color = \"purple\")\n",
    "plt.ylabel(\"Objective values\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.title(\"Objective Value vs Iteration Counter with Optimal Lambda\")\n",
    "plt.legend([\"Gradient Descent\", \"Fast Gradient Algorithm\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x174154b4a90>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot missclassification error on training set\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cvScore = cross_val_score(graddescent(X, y, stepSize = stepSize, targetAccuracy = epsilon, lamb = newLamb), X, y, scoring= \"accuracy\", cv=5)\n",
    "cvScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot misclassification error on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TALK ABOUT THIS!!!\n",
    "\n",
    "Lambda (λ) controls the trade-off between allowing the model to increase it's complexity as much as it wants with trying to keep it simple. For example, if λ is very low or 0, the model will have enough power to increase it's complexity (overfit) by assigning big values to the weights for each parameter. If, in the other hand, we increase the value of λ, the model will tend to underfit, as the model will become too simple. (https://www.kaggle.com/joparga3/2-tuning-parameters-for-logistic-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose we estimate the regression coefficients in a logistic regression model by minimizing $$F(\\beta) := \\frac{1}{n} \\sum_{i=1}^{n} \\log(1 + \\exp(-y_ix_i^T\\beta)) + \\lambda ||\\beta||_2^2$$ for a particular value of $\\lambda$. For parts (a) through (e), indicate which of (i) through (v) is correct. Justify your answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)  As we increase $\\lambda$ from 0, the misclassification error on the test set will:  \n",
    "(i) Increase initially, and then eventually start decreasing in an inverted U shape.  \n",
    "(ii) Decrease initially, and then eventually start increasing in a U shape.  \n",
    "(iii) Steadily increase.  \n",
    "(iv) Steadily decrease.  \n",
    "(v) Remain constant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. look at equation on slide 8, there is an ideal lambda value for which the model is general enough and then it gets too general and the misclassification error gets higher  \n",
    "\n",
    "bias variaince tradeoff and switches at local min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)  Repeat (a) for the misclassification error on the training set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. this is penalizing betas and if they are already optimized this will then:  \n",
    "\n",
    "Betas are already fit for this data so as your increase lambda the error will increase since worse fit for the training data exactly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) The fitting and testing times on both the CPU and GPU versions. You can find these in the output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"GPU\": (9.029633, 44.250862), \"CPU\": (1.102070, 982.011376)})\n",
    "df.rename(index={0:\"Fitting Time\", 1:\"Testing Time\"}, inplace = True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The url from the previous step (Check to make sure you successfully made it public!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My output file can be found here: https://s3.amazonaws.com/stat558-twilson/data558_astronomyoutput.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A statement of any problems you encountered during this exercise and how you overcame them (or if you didn’t).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got stuck in a few places, mainly with the unfamiliar Ubuntu file system and the AWS commands but was able to resolve them by searching Stack Overflow and similar sources for guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How long it took you to complete this exercise (for our reference–we’re not grading you on how long it took).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took me around 3 hours, but that includes time to familiarize myself with Ubuntu shell, AWS Console, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the data for the Kaggle competition. Run the script `extract_features.py` to extract features from the images. This script was written in Python 3 and depends on the library PyTorch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pick two classes of your choice from the dataset. Train an $l_2^2$-regularized logistic regression classifier on the training set using your own fast gradient algorithm with $\\lambda = 1$.Be sure to use the features you generated above rather than the raw image features. Plot, with different colors, the _misclassification error_ on the training set and on the validation set vs iterations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the value of the regularization parameter $\\lambda$ using cross-validation; you may use scikit-learn’s built-in functions for this purpose. Train an $l_2^2$-regularized logistic regression classifier on the training set using your own fast gradient algorithm with that value of $\\lambda$ found by cross-validation. Plot, with different colors, the _misclassification error_ on the training set and on the validation set vs iterations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite different coefficient values to correlated variables. We will now explore this property in a very simple setting.   \n",
    "Suppose that $n=2, p =2, x_{11}=x_{12}=x_{21}=x_{22}$. Furthermore, suppose that $y_1+y_2=0$ and $x_{11}+x_{21}=0$ and $x_{12}+x_{22}=0$, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: $\\hat{\\beta_0}=0$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Write out the ridge regression optimization problem in this setting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Argue that in this setting, the ridge coefficient estimates satisfy $\\beta_1 = \\beta_2$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
